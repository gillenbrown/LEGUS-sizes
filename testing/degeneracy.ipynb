{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit to fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from astropy import table, nddata\n",
    "from astropy.io import fits\n",
    "from scipy import optimize\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec, colors, ticker\n",
    "import cmocean\n",
    "import betterplotlib as bpl\n",
    "\n",
    "bpl.set_style()\n",
    "\n",
    "# need to add the correct path to import utils\n",
    "legus_home_dir = Path(\".\").resolve().parent\n",
    "sys.path.append(str(legus_home_dir / \"pipeline\"))\n",
    "import utils\n",
    "import fit_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling_factor = 2\n",
    "psf_size = 15\n",
    "snapshot_size = 30\n",
    "snapshot_size_oversampled = snapshot_size * oversampling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = legus_home_dir / \"data\" / \"ngc1313-w\"\n",
    "\n",
    "psf_name = f\"psf_my_stars_{psf_size}_pixels_{oversampling_factor}x_oversampled.fits\"\n",
    "psf = fits.open(data_dir / \"size\" / psf_name)[\"PRIMARY\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_params = [5,\n",
    "        30,\n",
    "        30,\n",
    "        1,\n",
    "        0.75,\n",
    "        1.25,\n",
    "        1.75,\n",
    "        123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = fit_utils.create_model_image(\n",
    "        *fake_params,\n",
    "        psf,\n",
    "        snapshot_size_oversampled,\n",
    "        oversampling_factor,\n",
    "    )[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chi_squared(params, cluster_snapshot, error_snapshot, mask):\n",
    "    \"\"\"\n",
    "    Calculate the chi-squared value for a given set of parameters.\n",
    "\n",
    "    :param params: Tuple of parameters of the EFF profile\n",
    "    :param cluster_snapshot: Cluster snapshot\n",
    "    :param error_snapshot: Error snapshot\n",
    "    :param mask: 2D array used as the mask, that contains 1 where there are pixels to\n",
    "                 use, and zero where the pixels are not to be used.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _, _, model_snapshot = fit_utils.create_model_image(\n",
    "        *params, psf, snapshot_size_oversampled, oversampling_factor\n",
    "    )\n",
    "    assert model_snapshot.shape == cluster_snapshot.shape\n",
    "    assert model_snapshot.shape == error_snapshot.shape\n",
    "\n",
    "    diffs = cluster_snapshot - model_snapshot\n",
    "    sigma_snapshot = diffs / error_snapshot\n",
    "    # then use the mask and the weights\n",
    "    sigma_snapshot *= mask\n",
    "    # do the radial weighting. Need to get the data coordinates of the center\n",
    "    sigma_snapshot *= fit_utils.radial_weighting(\n",
    "        cluster_snapshot,\n",
    "        fit_utils.oversampled_to_image(params[1], oversampling_factor),\n",
    "        fit_utils.oversampled_to_image(params[2], oversampling_factor),\n",
    "        style=\"annulus\",\n",
    "    )\n",
    "    return np.sum(np.abs(sigma_snapshot))\n",
    "\n",
    "def postprocess_params(log_mu_0, x_c, y_c, a, q, theta, eta, background):\n",
    "    \"\"\"\n",
    "    Postprocess the parameters, namely the axis ratio and position angle.\n",
    "\n",
    "    This is needed since we let the fit have axis ratios larger than 1, and position\n",
    "    angles of any value. Axis ratios larger than 1 indicate that we need to flip the\n",
    "    major and minor axes. This requires rotating the position angle 90 degrees, and\n",
    "    shifting the value assigned to the major axis to correct for the improper axis\n",
    "    ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    # q and a can be negative, fix that before any further processing\n",
    "    a = abs(a)\n",
    "    q = abs(q)\n",
    "    if q > 1.0:\n",
    "        q_final = 1.0 / q\n",
    "        a_final = a * q\n",
    "        theta_final = (theta - (np.pi / 2.0)) % np.pi\n",
    "        return log_mu_0, x_c, y_c, a_final, q_final, theta_final, eta, background\n",
    "    else:\n",
    "        return log_mu_0, x_c, y_c, a, q, theta % np.pi, eta, background\n",
    "\n",
    "\n",
    "def negative_log_likelihood(\n",
    "    params, cluster_snapshot, error_snapshot, mask\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the negative log likelihood for a model\n",
    "\n",
    "    We do the negative likelihood becuase scipy likes minimize rather than maximize,\n",
    "    so minimizing the negative likelihood is maximizing the likelihood\n",
    "\n",
    "    :param params: Tuple of parameters of the EFF profile\n",
    "    :param cluster_snapshot: Cluster snapshot\n",
    "    :param error_snapshot: Error snapshot\n",
    "    :param mask: 2D array used as the mask, that contains 1 where there are pixels to\n",
    "                 use, and zero where the pixels are not to be used.\n",
    "    :param estimated_bg: the estimated background value, to be used as the mean of the\n",
    "                         Gaussian prior on the background.\n",
    "    :param estimated_bg_sigma: the scatter in the estimated background. The sigma of the\n",
    "                               Gaussian prior on the background will be 0.1 times this.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # postprocess them from the beginning to make things simpler. This allows the\n",
    "    # fitting machinery to do what it wants, but under the hood we only use\n",
    "    # reasonable parameter value. This also handles the background scaling.\n",
    "    params = postprocess_params(*params)\n",
    "\n",
    "    chi_sq = calculate_chi_squared(params, cluster_snapshot, error_snapshot, mask)\n",
    "    log_data_likelihood = -chi_sq / 2.0\n",
    "    # Need to postprocess the parameters before calculating the prior, as the prior\n",
    "    # is on the physically reasonable values, we need to make sure that's correct.\n",
    "    log_prior = 0\n",
    "    log_likelihood = log_data_likelihood + log_prior\n",
    "    assert not np.isnan(log_prior)\n",
    "    assert not np.isnan(log_data_likelihood)\n",
    "    assert not np.isinf(log_prior)\n",
    "    assert not np.isinf(log_data_likelihood)\n",
    "    assert not np.isneginf(log_prior)\n",
    "    assert not np.isneginf(log_data_likelihood)\n",
    "    # return the negative of this so we can minimize this value\n",
    "    return -log_likelihood\n",
    "\n",
    "\n",
    "def format_exponent(log_a, pos):\n",
    "    assert np.isclose(float(log_a), int(log_a))\n",
    "    log_a = int(log_a)\n",
    "\n",
    "    if log_a > -2:\n",
    "        return str(10 ** log_a)\n",
    "    else:\n",
    "        return \"$10^{\" + f\"{log_a}\" + \"}$\"\n",
    "\n",
    "\n",
    "likelihood_cmap = cmocean.cm.haline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 1/20 [00:07<02:21,  7.47s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# Fit the background and peak value at each point, comparing with and without priors\n",
    "# ======================================================================================\n",
    "eta_min, eta_max, d_eta = (1.745, 1.755, 0.0005)\n",
    "a_min, a_max, d_a = (0.995, 1.005, 0.0005)\n",
    "\n",
    "eta_values = np.arange(eta_min, eta_max, d_eta)\n",
    "a_values = np.arange(a_min, a_max, d_a)\n",
    "\n",
    "n_eta = len(eta_values)\n",
    "n_a = len(a_values)\n",
    "\n",
    "# then make the output arrays\n",
    "log_likelihood = np.zeros((n_a, n_eta))\n",
    "\n",
    "for idx_eta in tqdm(range(n_eta)):\n",
    "    for idx_a in range(n_a):\n",
    "        eta = eta_values[idx_eta]\n",
    "        a = a_values[idx_a]\n",
    "\n",
    "        def chi_sq_wrapper(\n",
    "            to_fit\n",
    "        ):\n",
    "            return negative_log_likelihood(\n",
    "                (to_fit[0], fake_params[1], fake_params[2], a, fake_params[4], fake_params[5], eta, fake_params[7]),\n",
    "                fake_data,\n",
    "                np.ones(fake_data.shape),\n",
    "                np.ones(fake_data.shape),\n",
    "            )\n",
    "\n",
    "\n",
    "        # Find the best fit parameters\n",
    "        fit_params = optimize.minimize(\n",
    "            chi_sq_wrapper,\n",
    "            x0=(np.log10(np.max(fake_data) * 3)),\n",
    "            bounds=((None, 100),),\n",
    "        ).x\n",
    "        log_mu = fit_params[0]\n",
    "        \n",
    "#         print(a, eta, log_mu)\n",
    "        \n",
    "        best_fit_params = (log_mu, fake_params[1], fake_params[2], a, fake_params[4], fake_params[5], eta, fake_params[7])\n",
    "\n",
    "        # Then plug these into the likelihood function to see what we get\n",
    "        this_log_likelihood = -1 * negative_log_likelihood(\n",
    "            best_fit_params,\n",
    "            fake_data,\n",
    "            np.ones(fake_data.shape),\n",
    "            np.ones(fake_data.shape),\n",
    "        )\n",
    "\n",
    "\n",
    "        log_likelihood[idx_a, idx_eta] = this_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# plot this\n",
    "# ======================================================================================\n",
    "\n",
    "fig, ax = bpl.subplots()\n",
    "\n",
    "likelihood_width = 1\n",
    "\n",
    "vmax = np.max(log_likelihood)\n",
    "likelihood_norm = colors.Normalize(vmin=vmax - likelihood_width, vmax=vmax, clip=True)\n",
    "\n",
    "limits = (\n",
    "    min(eta_values) - 0.5 * d_eta,\n",
    "    max(eta_values) + 0.5 * d_eta,\n",
    "    min(a_values) - 0.5 * d_a,\n",
    "    max(a_values) + 0.5 * d_a,\n",
    ")\n",
    "i = ax.imshow(\n",
    "    log_likelihood,\n",
    "    origin=\"lower\",\n",
    "    extent=limits,\n",
    "    norm=likelihood_norm,\n",
    "    cmap=likelihood_cmap,\n",
    "    # This scalar aspect ratio is calculated to ensure the pixels are square\n",
    "    aspect=((eta_max - eta_min) / n_eta) / ((a_max - a_min) / n_a),\n",
    ")\n",
    "cbar = fig.colorbar(i, ax=ax)\n",
    "cbar.set_label(\"Log Likelihood = $-\\chi^2/2 +$ ln$(P(\\\\theta))$\")\n",
    "\n",
    "ax.set_limits(*limits)\n",
    "\n",
    "# mark the best fit point\n",
    "ax.scatter(\n",
    "    [fake_params[-2]],\n",
    "    [fake_params[3]],\n",
    "    marker=\"x\",\n",
    "    c=bpl.almost_black,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.add_labels(\"$\\eta$ (Power Law Slope)\", \"a (Scale Radius) [pixels]\")\n",
    "# ax.easy_add_text(f\"{galaxy.upper()} - {cluster_id}\", \"upper left\", color=\"white\")\n",
    "# fig.savefig(\n",
    "# Path(__file__).parent / f\"likelihood_contours_priors_{galaxy}_{cluster_id}.png\",\n",
    "# bbox_inches=\"tight\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legus",
   "language": "python",
   "name": "legus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
